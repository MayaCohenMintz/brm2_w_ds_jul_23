# lightning.pytorch==2.2.2
seed_everything: false
monitor: val/all/l1 # the metric to monitor. This is a string defined by the user, not a built in function
# from GPT: "This term suggests that the metric being monitored is the L1 loss (mean absolute 
# error) computed on the entire validation dataset. This metric is used to evaluate the model's performance during validation."
monitor_mode: min
trainer:
  precision: 16-mixed
  callbacks: 
    - class_path: ModelCheckpoint # checkpoint = version of the model.After training we take the checkpoint corresponding to the best performance. 
      init_args:
        monitor: ${monitor} # this is the monitor specified above 
        mode: ${monitor_mode}
        save_last: true
    - class_path: EarlyStopping
      init_args:
        monitor: ${monitor}
        mode: ${monitor_mode}
        patience: 20 # stops training if there's no improvement for 20 epochs 
    - LearningRateMonitor
    - class_path: RichProgressBar
      init_args:
        theme:
          metrics_text_delimiter: "\n"
  max_epochs: -1 # no limit 
  limit_train_batches: 1000 # limits the number of training batches per epoch 
model:
  backbone: # typically a pretrained NN model
    class_path: TimmBackbone ## class_path is the class responsible for handling the backbone.
    # TimmBackbone is a general interface provided by the timm deep learning library to handle various pretrained models.  
    init_args: # initialization arguments for TimmBackbone
      model_name: xception41
      pretrained: true
  backbone_finetuning:
    class_path: BackboneFinetuning
    init_args:
      unfreeze_backbone_at_epoch: 3
      backbone_initial_ratio_lr: 0.1
      verbose: true
data:
  data_sources:
    train: # these will be initializion params for a DataSource
    # MAYA - commented out the allen slices, put DS gt labels instead (all of them)
      - images_path: /home/ben/python/maya/brm2_with_deepslice_cloned-main/brm2/deepslice/images
        labels_path: /home/ben/python/maya/brm2_with_deepslice_cloned-main/brm2/deepslice/DS_gt_all_labels.csv 
        atlas_name: allen_mouse_25um
        type: real

      # - images_path: /home/ben/data/allen-resized 
      #   labels_path: /home/ben/python/brm2/data/allen/train_labels.csv
      #   atlas_name: allen_mouse_25um
      #   type: real
    val:
    # MAYA - commented out the allen slices, put DS gt validation labels instead
      - images_path: /home/ben/python/maya/brm2_with_deepslice_cloned-main/brm2/deepslice/images
        labels_path: /home/ben/python/maya/brm2_with_deepslice_cloned-main/brm2/deepslice/DS_gt_val_labels.csv
        atlas_name: allen_mouse_25um
        type: real
      # - images_path: /home/ben/data/allen-resized
      #   labels_path: /home/ben/python/brm2/data/allen/val_labels.csv
      #   atlas_name: allen_mouse_25um
      #   type: real
      # - images_path: /home/ben/python/brm2/data/deepslice_processed/images
      #   labels_path: /home/ben/python/brm2/data/deepslice_processed/val_labels.csv
      #   atlas_name: allen_mouse_25um
      #   type: real
    test:
    # MAYA - commented out the allen slices, put DS gt test labels instead
      - images_path: /home/ben/python/maya/brm2_with_deepslice_cloned-main/brm2/deepslice/images
        labels_path: /home/ben/python/maya/brm2_with_deepslice_cloned-main/brm2/deepslice/DS_gt_test_labels.csv
        atlas_name: allen_mouse_25um
        type: real
      # - images_path: /home/ben/data/allen-resized
      #   labels_path: /home/ben/python/brm2/data/allen/test_labels.csv
      #   atlas_name: allen_mouse_25um
      #   type: real
      # - images_path: /home/ben/python/brm2/data/deepslice_processed/images
      #   labels_path: /home/ben/python/brm2/data/deepslice_processed/test_labels.csv
      #   atlas_name: allen_mouse_25um
      #   type: real
  batch_size: 64 # batch size for data loading
  num_workers: 16
optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 1e-3 # learning rate
    betas: # the coefficients used for computing running averages of gradient and its square
    - 0.9
    - 0.999
    eps: 1.0e-08
    weight_decay: 0.0
# lr_scheduler:
#   class_path: lightning.pytorch.cli.ReduceLROnPlateau
#   init_args:
#     monitor: ${monitor}
#     mode: ${monitor_mode}
#     patience: 10
#     min_lr: 1e-4



